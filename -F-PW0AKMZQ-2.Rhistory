# Load the test data
load(system.file("extdata", "dyad_example_data.Rdata", package = "conversim"))
# Test preprocess_dyads function
test_that("preprocess_dyads works correctly", {
processed_data <- preprocess_dyads(dyad_example_data)
expect_is(processed_data, "data.frame")
expect_true("processed_text" %in% names(processed_data))
expect_true(all(nchar(processed_data$processed_text) > 0))
expect_true(all(grepl("^[a-z ]+$", processed_data$processed_text)))
})
# Test topic_sim_dyads function
test_that("topic_sim_dyads works correctly", {
processed_data <- preprocess_dyads(dyad_example_data)
result <- topic_sim_dyads(processed_data, method = "lda", num_topics = 2, window_size = 2)
expect_is(result, "list", info = "Result should be a list")
expected_names <- c("similarities_by_dyad", "overall_average", "model_summary", "anova_result")
actual_names <- names(result)
expect_true(all(expected_names %in% actual_names),
info = paste("Expected names:", paste(expected_names, collapse = ", "),
"\nActual names:", paste(actual_names, collapse = ", ")))
expect_is(result$similarities_by_dyad, "list",
info = "similarities_by_dyad should be a list")
expect_is(result$overall_average, "numeric",
info = "overall_average should be numeric")
expect_true(!is.na(result$overall_average),
info = "overall_average should not be NA")
expect_true(result$overall_average >= 0 && result$overall_average <= 1,
info = paste("overall_average should be between 0 and 1. Actual value:", result$overall_average))
# The model_summary and anova_result might be NULL if the mixed-effects model couldn't be fit
expect_true(is.null(result$model_summary) || is(result$model_summary, "summary.merMod"),
info = "model_summary should be NULL or a summary.merMod object")
expect_true(is.null(result$anova_result) || is(result$anova_result, "anova"),
info = "anova_result should be NULL or an anova object")
})
processed_data <- preprocess_dyads(dyad_example_data)
result <- topic_sim_dyads(processed_data[1:10,], method = "lda", num_topics = 2, window_size = 2)
#'   dyad_id = c(1, 1, 1, 1, 2, 2, 2, 2),
#'   speaker = c("A", "B", "A", "B", "C", "D", "C", "D"),
#'   processed_text = c("i love pizza", "me too favorite food",
#'                      "whats your favorite topping", "enjoy pepperoni mushrooms",
#'                      "i prefer pasta", "pasta delicious like spaghetti carbonara",
#'                      "ever tried making home", "yes quite easy make")
#' )
#' result <- topic_sim_dyads(convs, method = "lda", num_topics = 2, window_size = 2)
#' print(result$overall_average)
#' summary(result$model_summary)
topic_sim_dyads <- function(conversations, method = "lda", num_topics = 2, window_size = 3) {
if (!requireNamespace("lme4", quietly = TRUE)) {
stop("Package 'lme4' is required for this function. Please install it.")
}
dyads <- unique(conversations$dyad_id)
all_similarities <- list()
for (dyad in dyads) {
dyad_conv <- conversations[conversations$dyad_id == dyad, ]
similarities <- c()
for (i in 1:(nrow(dyad_conv) - window_size + 1)) {
window <- dyad_conv$processed_text[i:(i+window_size-1)]
corpus <- tm::Corpus(tm::VectorSource(window))
dtm <- tm::DocumentTermMatrix(corpus)
if (sum(slam::col_sums(dtm) > 0) == 0) {
similarities <- c(similarities, NA)
next
}
dtm <- dtm[slam::row_sums(dtm) > 0, ]
if (method == "lda") {
tryCatch({
lda_model <- topicmodels::LDA(dtm, k = num_topics, control = list(seed = 1234))
topics <- topicmodels::topics(lda_model)
sim <- sum(topics[1:(window_size/2)] == topics[(window_size/2+1):window_size]) / (window_size/2)
}, error = function(e) {
sim <- NA
})
} else {
stop("Unsupported method. Only 'lda' is currently implemented.")
}
similarities <- c(similarities, sim)
}
all_similarities[[as.character(dyad)]] <- similarities
}
# Prepare data for modeling
model_data <- data.frame(
dyad_id = rep(dyads, sapply(all_similarities, length)),
similarity = unlist(all_similarities)
)
model_data <- model_data[!is.na(model_data$similarity), ]
# Calculate overall average
overall_average <- mean(model_data$similarity, na.rm = TRUE)
# Try to fit the mixed-effects model
tryCatch({
if (length(unique(model_data$dyad_id)) > 1) {
model <- lme4::lmer(similarity ~ 1 + (1|dyad_id), data = model_data, control = lme4::lmerControl(check.nobs.vs.nlev = "ignore", check.nobs.vs.nRE = "ignore"))
# Check for convergence
if (!lme4::isSingular(model)) {
# Model converged, proceed with analysis
overall_average <- lme4::fixef(model)[1]
model_summary <- summary(model)
# Perform significance test
null_model <- lme4::lmer(similarity ~ 1 + (1|dyad_id), data = model_data, REML = FALSE, control = lme4::lmerControl(check.nobs.vs.nlev = "ignore", check.nobs.vs.nRE = "ignore"))
full_model <- lme4::lmer(similarity ~ 1 + (1|dyad_id), data = model_data, REML = FALSE, control = lme4::lmerControl(check.nobs.vs.nlev = "ignore", check.nobs.vs.nRE = "ignore"))
anova_result <- anova(null_model, full_model)
} else {
warning("Mixed-effects model is singular. Falling back to simple average.")
model_summary <- NULL
anova_result <- NULL
}
} else {
warning("Not enough dyads for mixed-effects model. Using simple average.")
model_summary <- NULL
anova_result <- NULL
}
}, error = function(e) {
warning("Failed to fit mixed-effects model. Using simple average. Error: ", e$message)
model_summary <- NULL
anova_result <- NULL
})
return(list(
similarities_by_dyad = all_similarities,
overall_average = overall_average,
model_summary = model_summary,
anova_result = anova_result
))
}
processed_data <- preprocess_dyads(dyad_example_data)
result <- topic_sim_dyads(processed_data[1:10,], method = "lda", num_topics = 2, window_size = 2)
length(unique(dyad_example_data))
length(unique(dyad_example_data$dyad_id))
# Test lexical_sim_dyads function
test_that("lexical_sim_dyads works correctly", {
processed_data <- preprocess_dyads(dyad_example_data)
result <- lexical_sim_dyads(processed_data, window_size = 2)
expect_is(result, "list")
expect_true(all(c("similarities_by_dyad", "overall_average", "model_summary", "anova_result") %in% names(result)))
expect_is(result$similarities_by_dyad, "list")
expect_is(result$overall_average, "numeric")
expect_true(result$overall_average >= 0 && result$overall_average <= 1)
})
# Test semantic_sim_dyads function
test_that("semantic_sim_dyads works correctly", {
processed_data <- preprocess_dyads(dyad_example_data)
result <- semantic_sim_dyads(processed_data, method = "tfidf", window_size = 2)
expect_is(result, "list")
expect_true(all(c("similarities_by_dyad", "overall_average", "model_summary", "anova_result") %in% names(result)))
expect_is(result$similarities_by_dyad, "list")
expect_is(result$overall_average, "numeric")
expect_true(result$overall_average >= 0 && result$overall_average <= 1)
})
# Test structural_sim_dyads function
test_that("structural_sim_dyads works correctly", {
processed_data <- preprocess_dyads(dyad_example_data)
result <- structural_sim_dyads(processed_data)
expect_is(result, "list")
expect_true(all(c("similarities_by_dyad", "overall_average", "confidence_interval", "t_test_result") %in% names(result)))
expect_is(result$similarities_by_dyad, "list")
expect_is(result$overall_average, "numeric")
expect_true(result$overall_average >= 0 && result$overall_average <= 1)
expect_length(result$confidence_interval, 2)
})
dyad_example_data.Rdata
dyad_example_data
# Load data
data_path <- system.file("extdata", "dyad_example_data.Rdata", package = "conversim")
load(data_path)
dyad_example_data
#' convs <- data.frame(
#'   dyad_id = c(1, 1, 1, 1, 2, 2, 2, 2),
#'   speaker = c("A", "B", "A", "B", "C", "D", "C", "D"),
#'   processed_text = c("i love pizza", "me too favorite food",
#'                      "whats your favorite topping", "enjoy pepperoni mushrooms",
#'                      "i prefer pasta", "pasta delicious like spaghetti carbonara",
#'                      "ever tried making home", "yes quite easy make")
#' )
#' semantic_sim_dyads(convs, method = "tfidf", window_size = 2)
#' @name semantic_sim_dyads
semantic_sim_dyads <- function(conversations, method = "tfidf", window_size = 3, ...) {
dyads <- unique(conversations$dyad_id)
all_similarities <- list()
for (dyad in dyads) {
dyad_conv <- conversations[conversations$dyad_id == dyad, ]
similarities <- c()
for (i in 1:(nrow(dyad_conv) - window_size + 1)) {
window1 <- paste(dyad_conv$processed_text[i:(i+window_size/2-1)], collapse = " ")
window2 <- paste(dyad_conv$processed_text[(i+window_size/2):(i+window_size-1)], collapse = " ")
sim <- conversim::semantic_similarity(window1, window2, method, ...)
similarities <- c(similarities, sim)
}
all_similarities[[as.character(dyad)]] <- similarities
}
# Prepare data for multilevel modeling
model_data <- data.frame(
dyad_id = rep(dyads, sapply(all_similarities, length)),
similarity = unlist(all_similarities)
)
# Fit multilevel model
model <- lme4::lmer(similarity ~ 1 + (1|dyad_id), data = model_data)
# Extract overall average similarity accounting for dyad-level variation
overall_average <- fixef(model)[1]
return(list(similarities_by_dyad = all_similarities, overall_average = overall_average))
}
convs <- data.frame(
dyad_id = c(1, 1, 1, 1, 2, 2, 2, 2),
speaker = c("A", "B", "A", "B", "C", "D", "C", "D"),
processed_text = c("i love pizza", "me too favorite food",
"whats your favorite topping", "enjoy pepperoni mushrooms",
"i prefer pasta", "pasta delicious like spaghetti carbonara",
"ever tried making home", "yes quite easy make")
)
gg <- semantic_sim_dyads(convs, method = "tfidf", window_size = 2)
#' convs <- data.frame(
#'   dyad_id = c(1, 1, 1, 1, 2, 2, 2, 2),
#'   speaker = c("A", "B", "A", "B", "C", "D", "C", "D"),
#'   processed_text = c("i love pizza", "me too favorite food",
#'                      "whats your favorite topping", "enjoy pepperoni mushrooms",
#'                      "i prefer pasta", "pasta delicious like spaghetti carbonara",
#'                      "ever tried making home", "yes quite easy make")
#' )
#' semantic_sim_dyads(convs, method = "tfidf", window_size = 2)
#' @name semantic_sim_dyads
semantic_sim_dyads <- function(conversations, method = "tfidf", window_size = 3, ...) {
dyads <- unique(conversations$dyad_id)
all_similarities <- list()
for (dyad in dyads) {
dyad_conv <- conversations[conversations$dyad_id == dyad, ]
similarities <- c()
for (i in 1:(nrow(dyad_conv) - window_size + 1)) {
window1 <- paste(dyad_conv$processed_text[i:(i+window_size/2-1)], collapse = " ")
window2 <- paste(dyad_conv$processed_text[(i+window_size/2):(i+window_size-1)], collapse = " ")
sim <- semantic_similarity(window1, window2, method, ...)
similarities <- c(similarities, sim)
}
all_similarities[[as.character(dyad)]] <- similarities
}
# Prepare data for multilevel modeling
model_data <- data.frame(
dyad_id = rep(dyads, sapply(all_similarities, length)),
similarity = unlist(all_similarities)
)
# Fit multilevel model
model <- lme4::lmer(similarity ~ 1 + (1|dyad_id), data = model_data)
# Extract overall average similarity accounting for dyad-level variation
overall_average <- fixef(model)[1]
return(list(similarities_by_dyad = all_similarities, overall_average = overall_average))
}
convs <- data.frame(
dyad_id = c(1, 1, 1, 1, 2, 2, 2, 2),
speaker = c("A", "B", "A", "B", "C", "D", "C", "D"),
processed_text = c("i love pizza", "me too favorite food",
"whats your favorite topping", "enjoy pepperoni mushrooms",
"i prefer pasta", "pasta delicious like spaghetti carbonara",
"ever tried making home", "yes quite easy make")
)
gg <- semantic_sim_dyads(convs, method = "tfidf", window_size = 2)
#' @return A numeric value representing the semantic similarity (between 0 and 1)
#' @export
#'
#' @importFrom tm Corpus VectorSource DocumentTermMatrix weightTfIdf
#' @importFrom word2vec word2vec
#' @name semantic_similarity
#' @examples
#' conv1 <- "The quick brown fox jumps over the lazy dog"
#' conv2 <- "A fast auburn canine leaps above an idle hound"
#' semantic_similarity(conv1, conv2, method = "tfidf")
semantic_similarity <- function(conversation1, conversation2, method = "tfidf", model_path = NULL, dim = 100, window = 5, iter = 5) {
# Internal function to calculate cosine similarity
cosine_similarity <- function(a, b) {
if (length(a) == 0 || length(b) == 0) return(0)
sim <- sum(a * b) / (sqrt(sum(a^2)) * sqrt(sum(b^2)))
# Ensure the result is between 0 and 1
return((sim + 1) / 2)
}
# Internal function to load pre-trained GloVe embeddings
load_glove <- function(file_path) {
tryCatch({
conn <- file(file_path, "r")
lines <- readLines(conn)
close(conn)
split_lines <- strsplit(lines, " ")
words <- sapply(split_lines, `[`, 1)
vectors <- t(sapply(split_lines, function(x) as.numeric(x[-1])))
rownames(vectors) <- words
return(vectors)
}, error = function(e) {
stop(paste("Error loading GloVe file:", e$message))
})
}
# Internal function to calculate sentence embedding
sentence_embedding <- function(sentence, word_vectors) {
tokens <- unlist(strsplit(sentence, "\\s+"))
valid_tokens <- tokens[tokens %in% rownames(word_vectors)]
if (length(valid_tokens) == 0) {
return(rep(0, ncol(word_vectors)))
}
embeddings <- word_vectors[valid_tokens, , drop = FALSE]
if (nrow(embeddings) == 0) return(rep(0, ncol(word_vectors)))
return(colMeans(embeddings))
}
if (method == "tfidf") {
# TF-IDF approach
corpus <- c(conversation1, conversation2)
dtm <- DocumentTermMatrix(Corpus(VectorSource(corpus)))
tfidf <- weightTfIdf(dtm)
m <- as.matrix(tfidf)
# Issue a warning for short conversations or little vocabulary overlap
if (nchar(conversation1) < 50 || nchar(conversation2) < 50 || ncol(m) < 5) {
warning("The 'tfidf' method may not provide highly meaningful results for short conversations or those with little vocabulary overlap. Consider using 'word2vec' or 'glove' methods for more robust results.")
}
# If the conversations are identical, return 1
if (identical(conversation1, conversation2)) {
return(1)
}
# Ensure we have at least one term in common
if (ncol(m) == 0) {
return(0)
}
# Calculate cosine similarity
similarity <- cosine_similarity(m[1,], m[2,])
} else if (method == "word2vec" || method == "glove") {
# Word2Vec or GloVe approach
if (method == "word2vec") {
# Train Word2Vec model
all_text <- c(conversation1, conversation2)
model <- word2vec(x = all_text, dim = dim, iter = iter, window = window, min_count = 1)
word_vectors <- as.matrix(model)
} else { # method == "glove"
if (is.null(model_path)) {
stop("Please provide a path to the pre-trained GloVe file.")
}
# Load pre-trained GloVe vectors
word_vectors <- load_glove(model_path)
}
# Calculate embeddings for each conversation
embedding1 <- sentence_embedding(conversation1, word_vectors)
embedding2 <- sentence_embedding(conversation2, word_vectors)
# Calculate cosine similarity
similarity <- cosine_similarity(embedding1, embedding2)
} else {
stop("Invalid method. Choose 'tfidf', 'word2vec', or 'glove'.")
}
return(similarity)
}
#' convs <- data.frame(
#'   dyad_id = c(1, 1, 1, 1, 2, 2, 2, 2),
#'   speaker = c("A", "B", "A", "B", "C", "D", "C", "D"),
#'   processed_text = c("i love pizza", "me too favorite food",
#'                      "whats your favorite topping", "enjoy pepperoni mushrooms",
#'                      "i prefer pasta", "pasta delicious like spaghetti carbonara",
#'                      "ever tried making home", "yes quite easy make")
#' )
#' semantic_sim_dyads(convs, method = "tfidf", window_size = 2)
#' @name semantic_sim_dyads
semantic_sim_dyads <- function(conversations, method = "tfidf", window_size = 3, ...) {
dyads <- unique(conversations$dyad_id)
all_similarities <- list()
for (dyad in dyads) {
dyad_conv <- conversations[conversations$dyad_id == dyad, ]
similarities <- c()
for (i in 1:(nrow(dyad_conv) - window_size + 1)) {
window1 <- paste(dyad_conv$processed_text[i:(i+window_size/2-1)], collapse = " ")
window2 <- paste(dyad_conv$processed_text[(i+window_size/2):(i+window_size-1)], collapse = " ")
sim <- semantic_similarity(window1, window2, method, ...)
similarities <- c(similarities, sim)
}
all_similarities[[as.character(dyad)]] <- similarities
}
# Prepare data for multilevel modeling
model_data <- data.frame(
dyad_id = rep(dyads, sapply(all_similarities, length)),
similarity = unlist(all_similarities)
)
# Fit multilevel model
model <- lme4::lmer(similarity ~ 1 + (1|dyad_id), data = model_data)
# Extract overall average similarity accounting for dyad-level variation
overall_average <- fixef(model)[1]
return(list(similarities_by_dyad = all_similarities, overall_average = overall_average))
}
convs <- data.frame(
dyad_id = c(1, 1, 1, 1, 2, 2, 2, 2),
speaker = c("A", "B", "A", "B", "C", "D", "C", "D"),
processed_text = c("i love pizza", "me too favorite food",
"whats your favorite topping", "enjoy pepperoni mushrooms",
"i prefer pasta", "pasta delicious like spaghetti carbonara",
"ever tried making home", "yes quite easy make")
)
gg <- semantic_sim_dyads(convs, method = "tfidf", window_size = 2)
devtools::load_all(".")
rm(list = c("semantic_sim_dyads", "semantic_similarity"))
semantic_sim_dyads
convs <- data.frame(
dyad_id = c(1, 1, 1, 1, 2, 2, 2, 2),
speaker = c("A", "B", "A", "B", "C", "D", "C", "D"),
processed_text = c("i love pizza", "me too favorite food",
"whats your favorite topping", "enjoy pepperoni mushrooms",
"i prefer pasta", "pasta delicious like spaghetti carbonara",
"ever tried making home", "yes quite easy make")
)
semantic_sim_dyads(convs, method = "tfidf", window_size = 2)
semantic_sim_dyads(dyad_example_data, method = "tfidf", window_size = 2)
semantic_sim_dyads(dyad_example_data, method = "tfidf", window_size = 4)
semantic_sim_dyads(dyad_example_data, method = "tfidf", window_size = 3)
lexical_sim_dyads(dyad_example_data, window_size = 2)
data_path <- system.file("extdata", "dyad_example_data.Rdata", package = "conversim")
load(data_path)
# Now, let's test functions from each file
# 1. Testing functions from conversation_multidyads.R
# Preprocess the conversations
preprocessed_data <- preprocess_dyads(dyad_example_data)
# Calculate topic similarity for multiple dyads
topic_sim_results <- topic_sim_dyads(preprocessed_data, method = "lda", num_topics = 3, window_size = 2)
# Calculate lexical similarity for multiple dyads
lexical_sim_results <- lexical_sim_dyads(preprocessed_data, window_size = 2)
# Calculate semantic similarity for multiple dyads
semantic_sim_results <- semantic_sim_dyads(preprocessed_data, method = "tfidf", window_size = 2)
# Calculate structural similarity for multiple dyads
structural_sim_results <- structural_sim_dyads(preprocessed_data)
# Calculate stylistic similarity for multiple dyads
stylistic_sim_results <- stylistic_sim_dyads(preprocessed_data)
# Calculate sentiment similarity for multiple dyads
sentiment_sim_results <- sentiment_sim_dyads(preprocessed_data)
# Calculate participant similarity for multiple dyads
participant_sim_results <- participant_sim_dyads(preprocessed_data)
# Calculate timing similarity for multiple dyads
timing_sim_results <- timing_sim_dyads(preprocessed_data)
cat("Topic Similarity in dyads:", topic_sim_results$overall_average, "\n")
cat("Lexical Similarity in dyads:", lexical_sim_results$overall_average, "\n")
cat("Semantic Similarity in dyads:", semantic_sim_results$overall_average, "\n")
cat("Structural Similarity in dyads:", structural_sim_results$overall_average, "\n")
cat("Stylistic Similarity in dyads:", stylistic_sim_results$overall_average, "\n")
cat("Sentiment Similarity in dyads:", sentiment_sim_results$overall_average, "\n")
cat("Participant Similarity in dyads:", participant_sim_results$overall_average, "\n")
cat("Timing Similarity in dyads:", timing_sim_results$overall_average, "\n")
# Select two conversations for comparison
conversation <- preprocessed_data %>% filter(dyad_id == 1) %>% select(speaker_id, processed_text)
library(dplyr)
# Select two conversations for comparison
conversation <- preprocessed_data %>% filter(dyad_id == 1) %>% select(speaker_id, processed_text)
# Calculate topic similarity sequence
topic_sim <- topic_sim_seq(conversation, method = "lda", num_topics = 2, window_size = 3)
## Lexical Similarity Sequence
lexical_sim <- lex_sim_seq(conversation, window_size = 3)
## Semantic Similarity Sequence
semantic_sim <- sem_sim_seq(conversation, method = "tfidf", window_size = 3)
## Stylistic Similarity Sequence
stylistic_sim <- style_sim_seq(conversation, window_size = 3)
## Sentiment Similarity Sequence
sentiment_sim <- sent_sim_seq(conversation, window_size = 3)
# Print results
cat("Topic Similarity:", topic_sim$sequence, "\n")
cat("Lexical Similarity:", lexical_sim$sequence, "\n")
cat("Semantic Similarity:", semantic_sim$sequence, "\n")
cat("Stylistic Similarity:", stylistic_sim$sequence, "\n")
cat("Sentiment Similarity:", sentiment_sim$sequence, "\n")
library(hexSticker)
install.packages("hexSticker")
library(hexSticker)
s <- sticker(~plot(cars, cex=.5, cex.axis=.5, mgp=c(0,.3,0), xlab="", ylab=""),
package="hexSticker", p_size=20, s_x=.8, s_y=.6, s_width=1.4, s_height=1.2,
filename="inst/figures/baseplot.png")
library(hexSticker)
s <- sticker(~plot(cars, cex=.5, cex.axis=.5, mgp=c(0,.3,0), xlab="", ylab=""),
package="hexSticker", p_size=20, s_x=.8, s_y=.6, s_width=1.4, s_height=1.2,
filename="D:\\Downloads\\baseplot.png")
library(hexSticker)
s <- sticker(~plot(cars, cex=.5, cex.axis=.5, mgp=c(0,.3,0), xlab="", ylab=""),
package="hexSticker", p_size=20, s_x=.8, s_y=.6, s_width=1.4, s_height=1.2,
filename="baseplot.png")
sticker(
subplot = "D:\\Downloads\\logo3.png",  # Replace with the path to your image
package = "YourPackageName",         # Replace with your package name
p_size = 20,                         # Font size of package name
p_color = "white",                   # Color of the package name text
s_x = 1, s_y = 0.75,                 # Position of the subplot (image) on x and y axis
s_width = 0.6, s_height = 0.6,       # Size of the subplot (image)
h_fill = "#1f77b4",                  # Background color of the sticker
h_color = "#ff7f0e",                 # Border color of the sticker
spotlight = TRUE,                    # Spotlight effect around the image
l_x = 1, l_y = 0.8,                  # Position of the spotlight (optional)
url = "yourwebsite.com",             # Add a URL to the sticker (optional)
u_size = 5,                          # Font size of the URL text
filename = "D:\\Downloads\\sticker.png" # Where to save the sticker
)
# Test preprocess_text function
preprocessed_A <- preprocess_text(speeches_data$text[1])
preprocessed_B <- preprocess_text(speeches_data$text[2])
library(conversim)
load(system.file("extdata", "dyad_example_data.Rdata", package = "conversim"))
load(system.file("extdata", "speeches_data.RData", package = "conversim"))
# Test preprocess_text function
preprocessed_A <- preprocess_text(speeches_data$text[1])
preprocessed_B <- preprocess_text(speeches_data$text[2])
# Test topic_similarity function
cat("Testing topic_similarity function:\n")
lda_similarity <- topic_similarity(speeches_data$text[1], speeches_data$text[2], method = "lda", num_topics = 5)
lsa_similarity <- topic_similarity(speeches_data$text[1], speeches_data$text[2], method = "lsa", num_topics = 5)
cat("LDA topic similarity:", lda_similarity, "\n")
cat("LSA topic similarity:", lsa_similarity, "\n\n")
# Test lexical_similarity function
cat("Testing lexical_similarity function:\n")
lex_similarity <- lexical_similarity(preprocessed_A, preprocessed_B)
cat("Lexical similarity:", lex_similarity, "\n\n")
# Test calculate_semantic_similarity function
cat("Testing calculate_semantic_similarity function:\n")
tfidf_similarity <- calculate_semantic_similarity(speeches_data$text[1], speeches_data$text[2], method = "tfidf")
devtools::build()
warnings()
semantic_similarity
devtools::load_all(".")
devtools::load_all(".")
devtools::document()
devtools::document()
devtools::load_all(".")
devtools::document()
devtools::build()
devtools::load_all(".")
